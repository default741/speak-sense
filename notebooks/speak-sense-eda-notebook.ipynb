{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeakSense - Language Detection System (Machine Learning CSCI 6364)\n",
    "\n",
    "**Abde Manaaf Ghadiali (G29583342), Gehna Ahuja (G00000000), Venkatesh Shanmugam (G00000000)**\n",
    "\n",
    "The objective of this project is to develop a robust and accurate system capable of detecting the language spoken in audio recordings. By leveraging advanced machine learning algorithms and signal processing techniques, the system aims to accurately identify the language spoken in various audio inputs, spanning diverse accents, dialects, and environmental conditions. This language detection solution seeks to provide practical applications in speech recognition, transcription, translation, and other fields requiring language-specific processing, thereby enhancing accessibility and usability across linguistic boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an environment for working with audio data, particularly focusing on Indian languages. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Importing Libraries**: Imports necessary libraries for data manipulation, visualization, machine learning, and audio processing.\n",
    "\n",
    "2. **Setting Display Options and Suppressing Warnings**: Configures display options for Pandas and suppresses warnings.\n",
    "\n",
    "3. **Setting Random Seed**: Sets a random seed for reproducibility.\n",
    "\n",
    "4. **Downloading Datasets**: Checks if the necessary datasets are downloaded, and if not, downloads them from Kaggle using the OpenDatasets library and organizes them into appropriate directories.\n",
    "\n",
    "5. **Audio Data Processing**: Prepares the audio data for further analysis. This might include feature extraction, preprocessing, and organizing the data for training machine learning models.\n",
    "\n",
    "6. **Machine Learning**: Utilizes machine learning techniques for tasks such as spoken language identification. This involves splitting the data into training and testing sets, building machine learning models (such as Random Forest or Gradient Boosting), evaluating the models, and generating classification reports and confusion matrices.\n",
    "\n",
    "7. **Deep Learning**: Utilizes deep learning techniques, specifically convolutional neural networks (CNNs), for tasks such as spoken language identification. This involves building and training deep learning models using the TensorFlow and Keras libraries.\n",
    "\n",
    "8. **Downloading Datasets from Kaggle**: This part downloads datasets from Kaggle using the OpenDatasets library. The datasets are related to audio data with Indian languages and spoken language identification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import opendatasets as od\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import librosa\n",
    "import pywt\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft, fftfreq\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_dict = {\n",
    "    'data_path': '../data',\n",
    "    'eda_results': '../data/eda_results',\n",
    "    'model_data': '../data/model_data',\n",
    "    'chunked_audio': '../data/model_data/chunked_audio',\n",
    "    'data_subset': '../data/model_data/data_subset',\n",
    "    'mfcc_dataframes': '../data/model_data/mfcc_dataframes',\n",
    "    'mfcc_mean_dataframes': '../data/model_data/mfcc_mean_dataframes',\n",
    "    'spectrogram_images': '../data/model_data/spectrogram_images',\n",
    "    'models': '../data/models'\n",
    "}\n",
    "\n",
    "for file_path_key in data_path_dict:\n",
    "    if not os.path.exists(data_path_dict[file_path_key]):\n",
    "        print(f'Path does not Exist: {data_path_dict[file_path_key]}')\n",
    "\n",
    "        os.makedirs(data_path_dict[file_path_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists('../data/audio-dataset-with-10-indian-languages') or os.path.exists('../data/audio_dataset_indian_languages')):\n",
    "    od.download(dataset_id_or_url=\"https://www.kaggle.com/datasets/hbchaitanyabharadwaj/audio-dataset-with-10-indian-languages\", data_dir='../data/')\n",
    "    os.rename('../data/audio-dataset-with-10-indian-languages/', '../data/audio_dataset_indian_languages/')\n",
    "\n",
    "if not (os.path.exists('../data/spoken-language-identification') or os.path.exists('../data/spoken_language_identification')):\n",
    "    od.download(dataset_id_or_url=\"https://www.kaggle.com/datasets/toponowicz/spoken-language-identification\", data_dir='../data/')\n",
    "    os.rename('../data/spoken-language-identification/', '../data/spoken_language_identification/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines variables related to file paths and sample filenames for audio datasets. Here's what each part does:\n",
    "\n",
    "1. `spoken_languages_train_path_dataset`: This variable holds the path to the training dataset for spoken language identification. It points to the directory where the audio files for training are stored.\n",
    "\n",
    "2. `indian_languages_train_path_dataset`: This variable holds the path to the training dataset for Indian languages. It points to the directory where the audio files for Indian languages are stored. The `*/*.mp3` part suggests that it's looking for all MP3 files within subdirectories.\n",
    "\n",
    "3. `filename_de`: This list contains sample filenames for the German language. Each filename seems to represent a fragment of an audio file. These filenames might be used for testing or validation purposes within the dataset.\n",
    "\n",
    "4. `filename_en`: This list contains sample filenames for the English language.\n",
    "\n",
    "5. `filename_es`: This list contains sample filenames for the Spanish language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_languages_train_path_dataset = '../data/spoken_language_identification/train/train/'\n",
    "indian_languages_train_path_dataset = '../data/audio_dataset_indian_languages/Language Detection Dataset/*/*.mp3'\n",
    "\n",
    "filename_de = ['de_f_0809fd0642232f8c85b0b3d545dc2b5a.fragment1.flac', 'de_f_5d2e7f30d69f2d1d86fd05f3bbe120c2.fragment1.flac']\n",
    "filename_en = ['en_f_058b70233667e1b64506dddf9f9d6b46.fragment1.flac', 'en_f_386ee651f6f1539ff5622c55e234e5a4.fragment3.flac']\n",
    "filename_es = ['es_f_47bd2e6178465cd745c86c9db5ffe447.fragment1.flac', 'es_f_ea5fee5b16a663c988fbddb2137cf573.fragment15.flac']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Audio Data, Sample Rate and Language of and Audio File and Play Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs the following tasks:\n",
    "\n",
    "1. **Loading Audio Data**:\n",
    "    ```python\n",
    "    data_de, sample_rate_de = librosa.load(spoken_languages_train_path_dataset + filename_de[0])\n",
    "    ```\n",
    "    It uses the `librosa.load()` function to load the audio file corresponding to the first filename in the `filename_de` list. The `spoken_languages_train_path_dataset` variable provides the directory path, and `filename_de[0]` provides the filename. The function returns two values: `data_de`, which contains the audio waveform data, and `sample_rate_de`, which represents the sampling rate of the audio file.\n",
    "\n",
    "2. **Displaying Audio Information**:\n",
    "    ```python\n",
    "    print(f'Audio for Language: German')\n",
    "    print(f'Audio Data Sample Rate: {sample_rate_de}')\n",
    "    print(f'Audio Data: {data_de}')\n",
    "    ```\n",
    "    It prints information about the loaded audio file. Specifically, it prints the language of the audio (German), the sample rate of the audio (`sample_rate_de`), and the audio data itself (`data_de`). This provides insight into the content and properties of the audio file.\n",
    "\n",
    "3. **Playing the Audio**:\n",
    "    ```python\n",
    "    ipd.Audio(data=data_de, rate=sample_rate_de)\n",
    "    ```\n",
    "    It uses `IPython.display.Audio()` to create an audio widget for playing the audio. It takes `data_de` as the audio data and `sample_rate_de` as the sample rate. When executed in a Jupyter Notebook environment, running this line will display an audio player widget that allows you to listen to the loaded audio file.\n",
    "\n",
    "Overall, this code snippet loads, displays information about, and plays an audio file for the German language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_de, sample_rate_de = librosa.load(spoken_languages_train_path_dataset + filename_de[0])\n",
    "\n",
    "print(f'Audio for Language: German')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_de}')\n",
    "print(f'Audio Data: {data_de}')\n",
    "\n",
    "ipd.Audio(data=data_de, rate=sample_rate_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en, sample_rate_en = librosa.load(spoken_languages_train_path_dataset + filename_en[0])\n",
    "\n",
    "print(f'Audio for Language: English')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_en}')\n",
    "print(f'Audio Data: {data_en}')\n",
    "\n",
    "ipd.Audio(data=data_en, rate=sample_rate_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_es, sample_rate_es = librosa.load(spoken_languages_train_path_dataset + filename_es[0])\n",
    "\n",
    "print(f'Audio for Language: Spanish')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_es}')\n",
    "print(f'Audio Data: {data_es}')\n",
    "\n",
    "ipd.Audio(data=data_es, rate=sample_rate_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bengali, sample_rate_bengali = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Bengali/0.mp3')\n",
    "\n",
    "print(f'Audio for Language: Bengali')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_bengali}')\n",
    "print(f'Audio Data: {data_bengali}')\n",
    "\n",
    "ipd.Audio(data=data_bengali, rate=sample_rate_bengali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gujarati, sample_rate_gujarati = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Gujarati/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Gujarati')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_gujarati}')\n",
    "print(f'Audio Data: {data_gujarati}')\n",
    "\n",
    "ipd.Audio(data=data_gujarati, rate=sample_rate_gujarati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hindi, sample_rate_hindi = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Hindi/0.mp3')\n",
    "\n",
    "print(f'Audio for Language: Hindi')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_hindi}')\n",
    "print(f'Audio Data: {data_hindi}')\n",
    "\n",
    "ipd.Audio(data=data_hindi, rate=sample_rate_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kannada, sample_rate_kannada = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Kannada/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Kannada')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_kannada}')\n",
    "print(f'Audio Data: {data_kannada}')\n",
    "\n",
    "ipd.Audio(data=data_kannada, rate=sample_rate_kannada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_malayalam, sample_rate_malayalam = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Malayalam/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Malayalam')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_malayalam}')\n",
    "print(f'Audio Data: {data_malayalam}')\n",
    "\n",
    "ipd.Audio(data=data_malayalam, rate=sample_rate_malayalam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_marathi, sample_rate_marathi = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Marathi/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Marathi')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_marathi}')\n",
    "print(f'Audio Data: {data_marathi}')\n",
    "\n",
    "ipd.Audio(data=data_marathi, rate=sample_rate_marathi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_punjabi, sample_rate_punjabi = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Punjabi/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Punjabi')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_punjabi}')\n",
    "print(f'Audio Data: {data_punjabi}')\n",
    "\n",
    "ipd.Audio(data=data_punjabi, rate=sample_rate_punjabi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tamil, sample_rate_tamil = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Tamil/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Tamil')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_tamil}')\n",
    "print(f'Audio Data: {data_tamil}')\n",
    "\n",
    "ipd.Audio(data=data_tamil, rate=sample_rate_tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_telugu, sample_rate_telugu = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Telugu/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Telugu')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_telugu}')\n",
    "print(f'Audio Data: {data_telugu}')\n",
    "\n",
    "ipd.Audio(data=data_telugu, rate=sample_rate_telugu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_urdu, sample_rate_urdu = librosa.load('../data/audio_dataset_indian_languages/Language Detection Dataset/Urdu/214.mp3')\n",
    "\n",
    "print(f'Audio for Language: Urdu')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_urdu}')\n",
    "print(f'Audio Data: {data_urdu}')\n",
    "\n",
    "ipd.Audio(data=data_urdu, rate=sample_rate_urdu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Amplitude Plots of Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `amplitude_plot_audio` for plotting the amplitude of audio data. Here's a breakdown of what it does:\n",
    "\n",
    "1. **Function Definition**:\n",
    "    ```python\n",
    "    def amplitude_plot_audio(data_dict: dict, n_rows: int = 1, n_cols: int = 3, figsize: tuple = (20, 5), file_name: str = 'amplitude_plot'):\n",
    "    ```\n",
    "    - The function `amplitude_plot_audio` takes several parameters:\n",
    "        - `data_dict`: A dictionary containing audio data where the keys are formatted as `<index>_<language>` where `<index>` represents the index of the audio data and `<language>` represents the language label.\n",
    "        - `n_rows`: Number of rows for subplots (default is 1).\n",
    "        - `n_cols`: Number of columns for subplots (default is 3).\n",
    "        - `figsize`: Size of the figure (default is (20, 5)).\n",
    "        - `file_name`: Name of the file to save the plot (default is 'amplitude_plot').\n",
    "\n",
    "2. **Plotting Amplitude**:\n",
    "    ```python\n",
    "    _, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "    ```\n",
    "    - This line creates subplots based on the specified number of rows (`n_rows`) and columns (`n_cols`). It returns a figure and an array of Axes objects (`ax`).\n",
    "\n",
    "3. **Iterating Over Data Dictionary**:\n",
    "    ```python\n",
    "    for key in data_dict.keys():\n",
    "        idx, lang = key.split('_')\n",
    "        idx = int(idx)\n",
    "\n",
    "        ax[idx].plot(data_dict[key])\n",
    "\n",
    "        ax[idx].set_ylabel('Amplitude')\n",
    "        ax[idx].set_xlabel('Time in samples')\n",
    "        ax[idx].set_title(f'Audio Amplitude vs Time ({lang})')\n",
    "    ```\n",
    "    - This loop iterates over each key in the `data_dict`, which represents the index and language label of each audio data.\n",
    "    - It splits each key to extract the index and language information.\n",
    "    - It plots the amplitude data (`data_dict[key]`) on the corresponding subplot (`ax[idx]`) and sets labels and titles accordingly.\n",
    "\n",
    "4. **Saving the Plot**:\n",
    "    ```python\n",
    "    plt.savefig(f'../data/eda_results/{file_name}.png')\n",
    "    ```\n",
    "    - After plotting all the amplitudes, the plot is saved as a PNG file in the directory `../data/eda_results/` with the specified `file_name`.\n",
    "\n",
    "Overall, this function generates a subplot for each audio sample, plotting its amplitude over time and labeling it with the corresponding language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitude_plot_audio(data_dict: dict, n_rows: int = 1, n_cols: int = 3, figsize: tuple = (20, 5), file_name: str = 'amplitude_plot') -> None:\n",
    "    \"\"\"Plots amplitude vs. time for audio data.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): Dictionary containing audio data. Keys are in the format 'idx_lang'.\n",
    "        n_rows (int, optional): Number of rows in the subplot grid. Default is 1.\n",
    "        n_cols (int, optional): Number of columns in the subplot grid. Default is 3.\n",
    "        figsize (tuple, optional): Figure size (width, height) in inches. Default is (20, 5).\n",
    "        file_name (str, optional): Name of the output file to save the plot. Default is 'amplitude_plot'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subplots with specified number of rows and columns\n",
    "    _, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "\n",
    "    # Iterate over keys in the data dictionary\n",
    "    for key in data_dict.keys():\n",
    "\n",
    "        # Split the key into index and language\n",
    "        idx, lang = key.split('_')\n",
    "        idx = int(idx)  # Convert index to integer\n",
    "\n",
    "        # Plot audio data on corresponding subplot\n",
    "        ax[idx].plot(data_dict[key])\n",
    "\n",
    "        # Set labels and title for the subplot\n",
    "        ax[idx].set_ylabel('Amplitude [dB]')\n",
    "        ax[idx].set_xlabel('Time in Samples [ms]')\n",
    "        ax[idx].set_title(f'Audio Amplitude vs Time ({lang})')\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f'../data/eda_results/{file_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filename_de)):\n",
    "    data_de, _ = librosa.load(spoken_languages_train_path_dataset + filename_de[i])\n",
    "    data_en, _ = librosa.load(spoken_languages_train_path_dataset + filename_en[i])\n",
    "    data_es, _ = librosa.load(spoken_languages_train_path_dataset + filename_es[i])\n",
    "\n",
    "    data_dict = {'0_de': data_de, '1_en': data_en, '2_es': data_es}\n",
    "\n",
    "    amplitude_plot_audio(data_dict=data_dict, file_name='amplitude_plot_de_en_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_languages_list = ['Hindi', 'Bengali', 'Gujarati']\n",
    "data_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/214.mp3')[0]\n",
    "\n",
    "amplitude_plot_audio(data_dict=data_dict, file_name='amplitude_plot_hbg')\n",
    "\n",
    "indian_languages_list = ['Kannada', 'Malayalam', 'Marathi']\n",
    "data_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/214.mp3')[0]\n",
    "\n",
    "amplitude_plot_audio(data_dict=data_dict, file_name='amplitude_plot_kmm')\n",
    "\n",
    "indian_languages_list = ['Punjabi', 'Tamil', 'Telugu', 'Urdu']\n",
    "data_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/214.mp3')[0]\n",
    "\n",
    "amplitude_plot_audio(data_dict=data_dict, n_rows=1, n_cols=4, figsize=(25, 5), file_name='amplitude_plot_pttu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Spectogram Plots of Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `spectogram_plot_audio` for plotting the spectrogram of audio data. Here's a breakdown of what it does:\n",
    "\n",
    "1. **Function Definition**:\n",
    "    ```python\n",
    "    def spectogram_plot_audio(data_dict: dict, sample_rate_dict: dict, n_rows: int = 1, n_cols: int = 3, figsize: tuple = (20, 5), file_name: str = 'spectogram_plot'):\n",
    "    ```\n",
    "    - The function `spectogram_plot_audio` takes several parameters:\n",
    "        - `data_dict`: A dictionary containing audio data where the keys are formatted as `<index>_<language>` where `<index>` represents the index of the audio data and `<language>` represents the language label.\n",
    "        - `sample_rate_dict`: A dictionary containing sample rates corresponding to each audio data in `data_dict`.\n",
    "        - `n_rows`: Number of rows for subplots (default is 1).\n",
    "        - `n_cols`: Number of columns for subplots (default is 3).\n",
    "        - `figsize`: Size of the figure (default is (20, 5)).\n",
    "        - `file_name`: Name of the file to save the plot (default is 'spectogram_plot').\n",
    "\n",
    "2. **Plotting Spectrogram**:\n",
    "    ```python\n",
    "    _, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "    ```\n",
    "    - This line creates subplots based on the specified number of rows (`n_rows`) and columns (`n_cols`). It returns a figure and an array of Axes objects (`ax`).\n",
    "\n",
    "3. **Iterating Over Data Dictionary**:\n",
    "    ```python\n",
    "    for key in data_dict.keys():\n",
    "        idx, lang = key.split('_')\n",
    "        idx = int(idx)\n",
    "\n",
    "        ax[idx].specgram(data_dict[key], Fs=sample_rate_dict[key])\n",
    "\n",
    "        ax[idx].set_ylabel('Frequency [Hz]')\n",
    "        ax[idx].set_xlabel('Time [sec]')\n",
    "        ax[idx].set_title(f'Audio Frequency vs Time ({lang})')\n",
    "    ```\n",
    "    - This loop iterates over each key in the `data_dict`, which represents the index and language label of each audio data.\n",
    "    - It splits each key to extract the index and language information.\n",
    "    - It plots the spectrogram of the audio data (`data_dict[key]`) on the corresponding subplot (`ax[idx]`) with the corresponding sample rate (`sample_rate_dict[key]`). It uses the `specgram` function from Matplotlib to generate the spectrogram.\n",
    "\n",
    "4. **Saving the Plot**:\n",
    "    ```python\n",
    "    plt.savefig(f'../data/eda_results/{file_name}.png')\n",
    "    ```\n",
    "    - After plotting all the spectrograms, the plot is saved as a PNG file in the directory `../data/eda_results/` with the specified `file_name`.\n",
    "\n",
    "Overall, this function generates a subplot for each audio sample, plotting its spectrogram and labeling it with the corresponding language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_plot_audio(data_dict: dict, sample_rate_dict: dict, n_rows: int = 1, n_cols: int = 3, figsize: tuple = (20, 5), file_name: str = 'spectrogram_plot') -> None:\n",
    "    \"\"\"Plots spectrogram for audio data.\n",
    "\n",
    "    Parameters:\n",
    "        data_dict (dict): Dictionary containing audio data. Keys are in the format 'idx_lang'.\n",
    "        sample_rate_dict (dict): Dictionary containing sample rates corresponding to audio data keys.\n",
    "        n_rows (int, optional): Number of rows in the subplot grid. Default is 1.\n",
    "        n_cols (int, optional): Number of columns in the subplot grid. Default is 3.\n",
    "        figsize (tuple, optional): Figure size (width, height) in inches. Default is (20, 5).\n",
    "        file_name (str, optional): Name of the output file to save the plot. Default is 'spectrogram_plot'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subplots with specified number of rows and columns\n",
    "    _, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "\n",
    "    # Iterate over keys in the data dictionary\n",
    "    for key in data_dict.keys():\n",
    "\n",
    "        # Split the key into index and language\n",
    "        idx, lang = key.split('_')\n",
    "        idx = int(idx)  # Convert index to integer\n",
    "\n",
    "        # Plot spectrogram of audio data on corresponding subplot\n",
    "        ax[idx].specgram(data_dict[key], Fs=sample_rate_dict[key])\n",
    "\n",
    "        # Set labels and title for the subplot\n",
    "        ax[idx].set_ylabel('Frequency [Hz]')\n",
    "        ax[idx].set_xlabel('Time [sec]')\n",
    "        ax[idx].set_title(f'Audio Frequency vs Time ({lang})')\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f'../data/eda_results/{file_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filename_de)):\n",
    "    data_de, samplerate_de = librosa.load(spoken_languages_train_path_dataset + filename_de[i])\n",
    "    data_en, samplerate_en = librosa.load(spoken_languages_train_path_dataset + filename_en[i])\n",
    "    data_es, samplerate_es = librosa.load(spoken_languages_train_path_dataset + filename_es[i])\n",
    "\n",
    "    data_dict = {'0_de': data_de, '1_en': data_en, '2_es': data_es}\n",
    "    sample_rate_dict = {'0_de': samplerate_de, '1_en': samplerate_en, '2_es': samplerate_es}\n",
    "\n",
    "    spectrogram_plot_audio(data_dict=data_dict, sample_rate_dict=sample_rate_dict, file_name='spectogram_plot_de_en_es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_languages_list = ['Hindi', 'Bengali', 'Gujarati']\n",
    "data_dict = {}\n",
    "sample_rate_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data, sample_rate = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/214.mp3')\n",
    "\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = data\n",
    "    sample_rate_dict[f'{idx}_{lang.lower()}'] = sample_rate\n",
    "\n",
    "spectrogram_plot_audio(data_dict=data_dict, sample_rate_dict=sample_rate_dict, file_name='spectogram_plot_hbg')\n",
    "\n",
    "indian_languages_list = ['Kannada', 'Malayalam', 'Marathi']\n",
    "audio_file_name = '214'\n",
    "data_dict = {}\n",
    "sample_rate_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data, sample_rate = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/{audio_file_name}.mp3')\n",
    "\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = data\n",
    "    sample_rate_dict[f'{idx}_{lang.lower()}'] = sample_rate\n",
    "\n",
    "spectrogram_plot_audio(data_dict=data_dict, sample_rate_dict=sample_rate_dict, file_name='spectogram_plot_kmm')\n",
    "\n",
    "indian_languages_list = ['Punjabi', 'Tamil', 'Telugu', 'Urdu']\n",
    "audio_file_name = '214'\n",
    "data_dict = {}\n",
    "sample_rate_dict = {}\n",
    "\n",
    "for idx, lang in enumerate(indian_languages_list):\n",
    "    data, sample_rate = librosa.load(f'../data/audio_dataset_indian_languages/Language Detection Dataset/{lang}/{audio_file_name}.mp3')\n",
    "\n",
    "    data_dict[f'{idx}_{lang.lower()}'] = data\n",
    "    sample_rate_dict[f'{idx}_{lang.lower()}'] = sample_rate\n",
    "\n",
    "spectrogram_plot_audio(data_dict=data_dict, sample_rate_dict=sample_rate_dict, n_rows=1, n_cols=4, figsize=(25, 5), file_name='spectogram_plot_pttu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Pre-Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function `load_data` to load audio data from a file and extract information about the sample rate and duration of the audio. Here's what it does:\n",
    "\n",
    "1. **Function Definition**:\n",
    "    ```python\n",
    "    def load_data(file_name: str) -> tuple:\n",
    "    ```\n",
    "    - The function `load_data` takes a single parameter `file_name`, which represents the path to the audio file to be loaded. It returns a tuple containing information about the sample rate and audio duration.\n",
    "\n",
    "2. **Loading Audio Data**:\n",
    "    ```python\n",
    "    audio_data, sample_rate = librosa.load(file_name, sr=None)\n",
    "    ```\n",
    "    - This line uses `librosa.load()` to load the audio file specified by `file_name`. The parameter `sr=None` indicates that the original sample rate of the audio file should be preserved. The function returns two values: `audio_data`, which contains the audio waveform data, and `sample_rate`, which represents the sampling rate of the audio file.\n",
    "\n",
    "3. **Calculating Audio Duration**:\n",
    "    ```python\n",
    "    audio_duration_sec = int(librosa.get_duration(y=audio_data, sr=sample_rate))\n",
    "    ```\n",
    "    - This line calculates the duration of the audio in seconds using `librosa.get_duration()`. It takes `audio_data` and `sample_rate` as parameters and returns the duration of the audio in seconds, which is then converted to an integer value.\n",
    "\n",
    "4. **Returning Results**:\n",
    "    ```python\n",
    "    return (sample_rate, audio_duration_sec)\n",
    "    ```\n",
    "    - The function returns a tuple containing the sample rate and audio duration.\n",
    "\n",
    "5. **Error Handling**:\n",
    "    ```python\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {str(e)}\")\n",
    "        return (np.nan, np.nan)\n",
    "    ```\n",
    "    - If an exception occurs during the loading or processing of the audio file, it prints an error message indicating the file name and the specific error. It then returns `(np.nan, np.nan)` to indicate that the sample rate and audio duration could not be determined.\n",
    "\n",
    "Overall, this function loads audio data from a file, calculates its sample rate and duration, and returns this information as a tuple. If an error occurs during the process, it prints an error message and returns NaN values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str) -> tuple:\n",
    "    \"\"\"Loads audio data from a file using librosa.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sample rate and duration of the audio in seconds.\n",
    "               If an error occurs during processing, returns (np.nan, np.nan).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load audio data and sample rate\n",
    "        audio_data, sample_rate = librosa.load(file_name, sr=None)\n",
    "\n",
    "        # Calculate duration of audio in seconds\n",
    "        audio_duration_sec = int(librosa.get_duration(y=audio_data, sr=sample_rate))\n",
    "\n",
    "        return (sample_rate, audio_duration_sec)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error message if an exception occurs during processing\n",
    "        print(f\"Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "        # Return NaN values for sample rate and duration\n",
    "        return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet performs several operations to create and preprocess a DataFrame containing information about audio files. Here's a breakdown:\n",
    "\n",
    "1. **Creating DataFrame for Spoken Languages**:\n",
    "    - It creates a DataFrame `spoken_language_dataframe` to store file names and language labels for audio files in the `spoken_languages_train_path_dataset` directory.\n",
    "    - It constructs the file paths by concatenating the directory path with each file name obtained using `os.listdir()`.\n",
    "    - It extracts the language label from the file name by splitting the string and taking the appropriate substring.\n",
    "    - It replaces language abbreviations ('de' for German, 'en' for English, 'es' for Spanish) with full language names.\n",
    "\n",
    "2. **Creating DataFrame for Indian Languages**:\n",
    "    - It creates a DataFrame `indian_language_dataframe` to store file names and language labels for audio files in the `indian_languages_train_path_dataset` directory.\n",
    "    - It uses `glob.glob()` to get all file names matching the pattern.\n",
    "    - It extracts the language label from the file name path.\n",
    "    - It filters out files labeled as 'punjabi'.\n",
    "\n",
    "3. **Combining DataFrames and Adding Additional Columns**:\n",
    "    - It concatenates the two DataFrames (`spoken_language_dataframe` and `indian_language_dataframe`) into a single DataFrame `language_dataframe`.\n",
    "    - It calculates the file size in kilobytes for each file and adds it as a new column.\n",
    "\n",
    "4. **Saving DataFrames to CSV Files**:\n",
    "    - It saves the `language_dataframe` to a CSV file named 'language_dataframe_v1.csv' without including the index column.\n",
    "\n",
    "5. **Processing Audio Data**:\n",
    "    - It iterates over unique language labels in the DataFrame.\n",
    "    - For each language, it creates a subset of the DataFrame containing only files for that language.\n",
    "    - It applies the `load_data` function to each file to extract sample rate and audio duration information and adds these as new columns in the subset DataFrame.\n",
    "    - It saves each subset DataFrame to a separate CSV file.\n",
    "\n",
    "6. **Combining Processed DataFrames**:\n",
    "    - It combines all processed subset DataFrames into a single DataFrame `language_dataframe`.\n",
    "    - It reads each CSV file for each language subset and concatenates them.\n",
    "    - It drops any rows with NaN values.\n",
    "\n",
    "7. **Saving Combined DataFrame to CSV File**:\n",
    "    - It saves the combined DataFrame to a CSV file named 'language_dataframe_v1.csv' without including the index column.\n",
    "\n",
    "Overall, this code prepares and preprocesses audio data and metadata, and stores it in CSV format for further analysis or use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_language_dataframe = pd.DataFrame({'file_name': [spoken_languages_train_path_dataset + file_name for file_name in os.listdir(spoken_languages_train_path_dataset)]})\n",
    "spoken_language_dataframe['language_label'] = spoken_language_dataframe['file_name'].str.split('/', expand=True).iloc[:, 5].str.split('_', expand=True).iloc[:, 0].replace(to_replace={\n",
    "    'de': 'german', 'en': 'english', 'es': 'spanish'})\n",
    "\n",
    "indian_language_dataframe = pd.DataFrame({'file_name': glob.glob(indian_languages_train_path_dataset)})\n",
    "indian_language_dataframe['language_label'] = indian_language_dataframe['file_name'].str.split('\\\\', expand=True).iloc[:, 1].str.lower()\n",
    "\n",
    "indian_language_dataframe = indian_language_dataframe[indian_language_dataframe['language_label'] != 'punjabi']\n",
    "\n",
    "language_dataframe = pd.concat([spoken_language_dataframe, indian_language_dataframe], ignore_index=True)\n",
    "language_dataframe['file_size_kb'] = (language_dataframe['file_name'].apply(lambda x: os.path.getsize(x)) / 1024).round(3)\n",
    "\n",
    "language_dataframe.to_csv('../data/model_data/language_dataframe_v1.csv', index=False)\n",
    "\n",
    "for lang in tqdm(language_dataframe['language_label'].unique(), desc=\"Languages\"):\n",
    "    lang_data = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "    lang_data[['sample_rate', 'audio_duration_sec']] = lang_data['file_name'].apply(lambda file_name: pd.Series(load_data(file_name=file_name)))\n",
    "\n",
    "    lang_data.to_csv(f'../data/model_data/data_subset/language_dataframe_{lang}_v1.csv', index=False)\n",
    "\n",
    "language_dataframe = pd.concat([pd.read_csv(f'../data/model_data/data_subset/language_dataframe_{lang}_v1.csv') for lang in language_dataframe['language_label'].unique()], ignore_index=True).dropna()\n",
    "language_dataframe.to_csv('../data/model_data/language_dataframe_v1.csv', index=False)\n",
    "\n",
    "language_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics of Raw Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe = pd.read_csv('../data/model_data/language_dataframe_v1.csv')\n",
    "language_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['sample_rate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['file_size_kb'].mean(), language_dataframe['file_size_kb'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['audio_duration_sec'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waveform Visualization\n",
    "\n",
    "Waveform visualization is a technique used to plot the amplitude of an audio signal against time. This type of visualization provides a graphical representation of the audio waveform, allowing us to visualize the variations in amplitude over time. Each point on the waveform represents the magnitude of the audio signal at a specific point in time.\n",
    "\n",
    "Waveform visualization is useful for gaining insights into the overall structure and dynamics of the audio. By examining the waveform, we can identify patterns, such as changes in amplitude, duration of sounds, and presence of silence or noise. This can help in understanding the rhythm, tempo, and intensity of the audio.\n",
    "\n",
    "In addition, waveform visualization is commonly used for tasks such as audio editing, quality assessment, and transcription. It provides a simple yet informative representation of the audio signal, making it easier to analyze and interpret the characteristics of the audio.\n",
    "\n",
    "Overall, waveform visualization is a fundamental tool in audio processing and analysis, offering valuable insights into the temporal dynamics of the audio signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = language_dataframe['file_name'].iloc[0]\n",
    "language = language_dataframe['language_label'].iloc[0]\n",
    "\n",
    "audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y=audio_data, sr=sample_rate)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Amplitude [db]')\n",
    "plt.title(f'Waveform Plot ({language.capitalize()})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectrogram Visualization\n",
    "\n",
    "A spectrogram is a visual representation of the frequency content of an audio signal over time. It is created by computing the Short-Time Fourier Transform (STFT) of the audio signal, which divides the signal into short overlapping segments and computes the Fourier Transform for each segment. The resulting spectrogram displays the magnitude of the frequency components as a function of time.\n",
    "\n",
    "Spectrogram visualization is a powerful tool for analyzing audio signals, as it provides insights into various aspects of the signal's characteristics. By examining the spectrogram, one can identify patterns such as frequency components, changes in frequency content over time, and the presence of specific sounds or events.\n",
    "\n",
    "The spectrogram is typically displayed with time on the x-axis, frequency on the y-axis, and color representing the magnitude of the frequency components. Brighter colors indicate higher magnitudes, while darker colors indicate lower magnitudes.\n",
    "\n",
    "Spectrogram visualization is widely used in various audio processing tasks, including speech recognition, music analysis, sound classification, and environmental monitoring. It enables researchers and practitioners to explore the frequency dynamics of audio signals and extract meaningful information from them.\n",
    "\n",
    "Overall, spectrogram visualization is an essential technique for gaining insights into the frequency content and temporal dynamics of audio signals, making it a valuable tool in audio analysis and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.stft(audio_data)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(S_db, sr=sample_rate, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-frequency Cepstral Coefficients (MFCCs)\n",
    "\n",
    "Mel-frequency Cepstral Coefficients (MFCCs) are a set of features widely used in speech and audio processing tasks. They represent the short-term power spectrum of a sound in a compact and perceptually relevant manner. MFCCs are derived from the Mel-frequency cepstrum, which is a representation of the short-term power spectrum of a sound after it has been passed through a series of filters designed to mimic the human auditory system's response.\n",
    "\n",
    "The process of extracting MFCCs involves several steps:\n",
    "\n",
    "1. **Frame the Signal**: The audio signal is divided into short overlapping frames, typically ranging from 20 to 40 milliseconds in duration.\n",
    "\n",
    "2. **Apply a Window Function**: A window function, such as the Hamming window, is applied to each frame to reduce spectral leakage.\n",
    "\n",
    "3. **Compute the Fourier Transform**: The Fourier Transform is computed for each framed signal to obtain its frequency spectrum.\n",
    "\n",
    "4. **Apply the Mel Filterbank**: The Mel Filterbank is a series of triangular filters spaced evenly in the Mel-frequency scale, which is a perceptually relevant scale of frequency. The energy in each filter's frequency band is computed.\n",
    "\n",
    "5. **Take the Logarithm**: The logarithm of the energy in each filterbank is computed to better approximate the human auditory system's response to sound.\n",
    "\n",
    "6. **Compute the Discrete Cosine Transform (DCT)**: The DCT is applied to the log-filterbank energies to decorrelate the features and obtain the MFCCs.\n",
    "\n",
    "The resulting MFCCs capture the spectral characteristics of the audio signal in a compact form, making them suitable for tasks such as speech recognition, speaker identification, and audio classification. They are particularly effective in capturing phonetic and speaker-related information from speech signals.\n",
    "\n",
    "Overall, MFCCs are a powerful feature representation for audio signals, providing a robust and efficient way to characterize the spectral content of sound.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Mel-frequency Cepstral Coefficients')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy Distribution Analysis\n",
    "\n",
    "Energy distribution analysis involves examining how the energy of an audio signal is distributed across different frequency bands. This analysis can provide insights into the dominant frequencies or energy patterns present in the audio signal.\n",
    "\n",
    "The process of energy distribution analysis typically involves the following steps:\n",
    "\n",
    "1. **Compute the Short-Time Fourier Transform (STFT)**: The audio signal is divided into short overlapping segments, and the Fourier Transform is computed for each segment. This results in a time-frequency representation of the signal, where the magnitude of the frequency components is represented as a function of time.\n",
    "\n",
    "2. **Partition the Frequency Spectrum**: The frequency spectrum obtained from the STFT is divided into multiple frequency bands. These bands can be equally spaced or based on perceptual scales such as the Mel-frequency scale.\n",
    "\n",
    "3. **Calculate Energy in Each Band**: The energy of the signal within each frequency band is computed by summing the magnitudes of the frequency components within that band. This provides a measure of the signal's power within each frequency range.\n",
    "\n",
    "4. **Visualize the Energy Distribution**: The energy distribution across different frequency bands is visualized using a histogram, bar plot, or heat map. This visualization allows for the identification of dominant frequencies or energy patterns present in the audio signal.\n",
    "\n",
    "Energy distribution analysis is commonly used in various audio processing tasks, including music analysis, speech recognition, and sound classification. It can help in identifying key characteristics of the audio signal and extracting relevant features for further analysis.\n",
    "\n",
    "Overall, energy distribution analysis provides valuable insights into the frequency content and power distribution of audio signals, facilitating a better understanding of their underlying properties and behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "\n",
    "stft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\n",
    "power_spec = np.abs(stft) ** 2\n",
    "\n",
    "num_bands = 10\n",
    "freq_bands = np.linspace(0, sample_rate / 2, num_bands + 1).astype(int)\n",
    "energy_distribution = np.zeros((num_bands, power_spec.shape[1]))\n",
    "\n",
    "for i in range(num_bands):\n",
    "    energy_distribution[i, :] = np.sum(power_spec[freq_bands[i]:freq_bands[i+1], :], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(energy_distribution, aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar(label='Energy')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('Frequency Band')\n",
    "plt.title('Energy Distribution Across Frequency Bands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitch Analysis\n",
    "\n",
    "Pitch analysis involves estimating the fundamental frequency or pitch of an audio signal, which represents the perceived frequency of the sound. Pitch is a fundamental perceptual attribute of sound and is closely related to the frequency of the sound wave.\n",
    "\n",
    "The process of pitch analysis typically involves the following steps:\n",
    "\n",
    "1. **Preprocessing**: The audio signal may undergo preprocessing steps such as filtering, windowing, and normalization to improve the accuracy of pitch estimation.\n",
    "\n",
    "2. **Pitch Estimation**: Various algorithms can be used to estimate the pitch of the audio signal. One common method is the autocorrelation-based technique, which involves computing the autocorrelation function of the signal and identifying the peaks corresponding to the fundamental frequency.\n",
    "\n",
    "3. **Post-processing**: Post-processing steps may be applied to refine the estimated pitch values and eliminate spurious or erroneous detections.\n",
    "\n",
    "Pitch analysis is useful in a wide range of applications, including music transcription, speech processing, voice analysis, and emotion recognition. In music transcription, pitch analysis is used to convert audio recordings into symbolic representations of music, such as sheet music or MIDI files. In speech processing, pitch analysis can provide insights into the prosodic features of speech, such as intonation and emphasis. In voice analysis, pitch analysis can help identify characteristics such as gender and age.\n",
    "\n",
    "Overall, pitch analysis plays a crucial role in understanding and interpreting the frequency characteristics of audio signals, making it a valuable tool in various audio processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch = librosa.pyin(audio_data, fmin=librosa.note_to_hz('C1'), fmax=librosa.note_to_hz('C8'))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(librosa.frames_to_time(range(len(pitch)), sr=sample_rate), pitch, color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Pitch (Hz)')\n",
    "plt.title('Pitch Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Features\n",
    "\n",
    "Temporal features are statistical measures computed over the temporal domain of an audio signal. These features provide insights into the temporal characteristics of the audio, capturing various aspects of its amplitude and distribution over time.\n",
    "\n",
    "Some commonly computed temporal features include:\n",
    "\n",
    "- **Mean**: The mean represents the average amplitude of the audio signal over time. It provides a measure of the central tendency of the signal's amplitude distribution.\n",
    "\n",
    "- **Variance**: The variance measures the spread or dispersion of the amplitude values around the mean. It indicates the degree of variability or fluctuation in the signal's amplitude over time.\n",
    "\n",
    "- **Skewness**: Skewness quantifies the asymmetry of the amplitude distribution. A positive skewness value indicates that the tail of the distribution is skewed towards higher values, while a negative skewness value indicates skewness towards lower values.\n",
    "\n",
    "- **Kurtosis**: Kurtosis measures the \"tailedness\" of the amplitude distribution. It reflects the degree to which the distribution is peaked or flat compared to a normal distribution. High kurtosis values indicate a sharper peak and heavier tails, while low kurtosis values indicate a flatter distribution.\n",
    "\n",
    "Computing these temporal features provides valuable insights into the overall shape, variability, and distribution of the audio signal over time. These features are commonly used in various audio processing tasks, including audio classification, event detection, and anomaly detection.\n",
    "\n",
    "By analyzing temporal features, researchers and practitioners can gain a better understanding of the temporal characteristics of the audio signal and extract meaningful information for further analysis and interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(audio_data)\n",
    "variance = np.var(audio_data)\n",
    "\n",
    "skewness = np.mean((audio_data - np.mean(audio_data)) ** 3) / np.mean((audio_data - np.mean(audio_data)) ** 2) ** (3/2)\n",
    "kurtosis = np.mean((audio_data - np.mean(audio_data)) ** 4) / np.mean((audio_data - np.mean(audio_data)) ** 2) ** 2\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", variance)\n",
    "print(\"Skewness:\", skewness)\n",
    "print(\"Kurtosis:\", kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Domain Analysis\n",
    "\n",
    "Frequency domain analysis involves analyzing the frequency spectrum of an audio signal using techniques such as the Fourier Transform or Short-Time Fourier Transform (STFT). This analysis provides insights into the frequency components present in the audio signal and how they vary over time.\n",
    "\n",
    "The Fourier Transform is a mathematical technique that decomposes a signal into its constituent frequency components. By applying the Fourier Transform to an audio signal, we can obtain a representation of its frequency spectrum, which shows the magnitude and phase of each frequency component.\n",
    "\n",
    "The Short-Time Fourier Transform (STFT) extends the Fourier Transform to analyze the frequency content of a signal over short time intervals. It divides the signal into overlapping segments, computes the Fourier Transform for each segment, and then combines the results to create a time-frequency representation of the signal. This allows us to observe how the frequency components of the signal change over time.\n",
    "\n",
    "Frequency domain analysis can reveal important information about the characteristics of the audio signal, such as:\n",
    "\n",
    "- Dominant frequencies: The main frequency components present in the signal.\n",
    "- Harmonic structure: The relationship between the fundamental frequency and its harmonics.\n",
    "- Transient events: Short-duration changes in the frequency content of the signal.\n",
    "- Spectral patterns: Repeating patterns or motifs in the frequency spectrum.\n",
    "\n",
    "Frequency domain analysis is widely used in various audio processing tasks, including audio compression, noise reduction, and speech enhancement. It provides a powerful tool for understanding the spectral properties of audio signals and extracting meaningful information for further analysis and processing.\n",
    "\n",
    "Overall, frequency domain analysis is an essential technique in audio signal processing, enabling researchers and practitioners to explore the frequency characteristics of audio signals and uncover valuable insights about their underlying structure and dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "stft = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "stft_db = librosa.amplitude_to_db(np.abs(stft))\n",
    "freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(stft_db, sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='hz')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Frequency Spectrum')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(audio_data)\n",
    "xf = fft(audio_data)\n",
    "xf_freqs = fftfreq(N, (1.0 / sample_rate))[:N//2]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(xf_freqs, 2.0 / N * np.abs(xf[:N//2]))\n",
    "plt.title('Frequency Spectrum')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Segmentation\n",
    "\n",
    "Audio segmentation is the process of dividing an audio signal into meaningful segments or parts based on various features such as silence detection, changes in energy, or spectral characteristics. This technique helps in identifying different sections or events within the audio, making it easier to analyze and process the signal.\n",
    "\n",
    "There are several methods for audio segmentation, including:\n",
    "\n",
    "1. **Silence Detection**: Silence detection involves identifying periods of silence or low energy in the audio signal. This can be achieved by setting a threshold on the signal's energy level and detecting segments with energy below the threshold.\n",
    "\n",
    "2. **Energy-Based Segmentation**: Energy-based segmentation focuses on detecting changes in the energy level of the audio signal. Segments with significant changes in energy levels may correspond to different events or sections within the audio.\n",
    "\n",
    "3. **Spectral Characteristics**: Spectral-based segmentation involves analyzing the frequency content of the audio signal to identify segments with distinct spectral characteristics. This can be done using techniques such as clustering or pattern recognition based on features extracted from the frequency domain.\n",
    "\n",
    "4. **Machine Learning Approaches**: Machine learning techniques can also be used for audio segmentation, where models are trained to classify segments based on various features extracted from the audio signal.\n",
    "\n",
    "Audio segmentation is a crucial step in many audio processing tasks, including speech recognition, music transcription, and sound event detection. It allows for the identification and isolation of relevant sections within the audio signal, enabling more focused analysis and processing.\n",
    "\n",
    "By segmenting the audio signal into meaningful parts, researchers and practitioners can better understand its structure and content, leading to more accurate and effective processing outcomes.\n",
    "\n",
    "Overall, audio segmentation plays a vital role in audio processing applications, facilitating the extraction of valuable information and insights from audio signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "hop_length = 512\n",
    "energy = np.array([sum(abs(audio_data[i:i+window_size] ** 2)) for i in range(0, len(audio_data), hop_length)])\n",
    "\n",
    "threshold = np.mean(energy) * 0.1\n",
    "silence_segments = np.where(energy < threshold)[0]\n",
    "\n",
    "segments = []\n",
    "start = 0\n",
    "\n",
    "for silence in silence_segments:\n",
    "    segments.append((start, silence * hop_length))\n",
    "    start = silence * hop_length\n",
    "\n",
    "for i, (start, end) in enumerate(segments):\n",
    "    print(f\"Segment {i + 1}: Start = {start / sample_rate:.3f} seconds, End = {end / sample_rate:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering and Dimensionality Reduction\n",
    "\n",
    "Clustering techniques and dimensionality reduction methods are powerful tools used to explore the structure of audio data and discover similarities or patterns among audio samples. These methods help in organizing large audio datasets, identifying clusters of similar audio samples, and reducing the dimensionality of the data while preserving its essential characteristics.\n",
    "\n",
    "##### Clustering Techniques\n",
    "\n",
    "Clustering techniques group similar audio samples together based on their feature representations. Some commonly used clustering algorithms include K-means clustering, hierarchical clustering, and density-based clustering. These algorithms partition the data into clusters such that audio samples within the same cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "##### Dimensionality Reduction Methods\n",
    "\n",
    "Dimensionality reduction methods aim to reduce the number of features in the audio data while preserving as much information as possible. Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are popular dimensionality reduction techniques used in audio processing. PCA identifies the principal components (linear combinations of the original features) that capture the most variance in the data. t-SNE, on the other hand, is a non-linear technique that projects high-dimensional data into a lower-dimensional space, emphasizing the local structure of the data.\n",
    "\n",
    "##### Application in Audio Processing\n",
    "\n",
    "Clustering and dimensionality reduction techniques are widely used in various audio processing tasks, including:\n",
    "\n",
    "- **Audio Classification**: Clustering can help in grouping audio samples with similar characteristics, making it easier to classify them into different categories or classes.\n",
    "- **Content-Based Retrieval**: Dimensionality reduction techniques can be used to create compact representations of audio data, enabling efficient retrieval of similar audio samples from large databases.\n",
    "- **Audio Visualization**: Clustering and dimensionality reduction methods can aid in visualizing high-dimensional audio data in lower-dimensional spaces, allowing for intuitive exploration and interpretation of the data.\n",
    "\n",
    "By applying clustering and dimensionality reduction techniques to audio data, researchers and practitioners can gain valuable insights into the underlying structure and patterns of the data, facilitating various audio processing tasks and applications.\n",
    "\n",
    "Overall, clustering and dimensionality reduction are essential tools in audio processing, enabling the exploration and analysis of large audio datasets and the discovery of meaningful patterns and similarities among audio samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "mfccs_pca = pca.fit_transform(mfccs.T)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(mfccs_pca)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for cluster in range(kmeans.n_clusters):\n",
    "    plt.scatter(mfccs_pca[clusters == cluster, 0], mfccs_pca[clusters == cluster, 1], label=f'Cluster {cluster + 1}')\n",
    "\n",
    "plt.title('Clustering of Audio Segments (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-Frequency Decomposition\n",
    "\n",
    "Time-frequency decomposition is a technique used to decompose an audio signal into representations that simultaneously capture both time and frequency characteristics. This allows for a more detailed analysis of how the frequency content of the signal evolves over time. Two common techniques for time-frequency decomposition are the wavelet transform and the Wigner-Ville distribution.\n",
    "\n",
    "##### Wavelet Transform\n",
    "\n",
    "The wavelet transform is a mathematical tool that decomposes a signal into a set of wavelets, which are small wave-like functions that are localized in both time and frequency. By applying the wavelet transform to an audio signal, we can obtain a time-frequency representation that highlights both short-term and long-term frequency components of the signal. This representation is particularly useful for analyzing transient signals or signals with rapidly changing frequency content.\n",
    "\n",
    "##### Wigner-Ville Distribution\n",
    "\n",
    "The Wigner-Ville distribution is a time-frequency representation obtained by computing the joint time-frequency distribution of a signal. It provides a high-resolution representation of the signal's time-frequency content, capturing both instantaneous and non-instantaneous frequency components. However, the Wigner-Ville distribution may suffer from cross-term interference, which can introduce artifacts in the representation.\n",
    "\n",
    "##### Application in Audio Processing\n",
    "\n",
    "Time-frequency decomposition techniques are widely used in various audio processing tasks, including:\n",
    "\n",
    "- **Audio Analysis**: Time-frequency representations allow for a detailed analysis of the frequency content of audio signals over time, facilitating tasks such as event detection, pitch tracking, and sound classification.\n",
    "- **Audio Compression**: Time-frequency representations can be used to identify and remove redundant or irrelevant information from audio signals, leading to more efficient compression algorithms.\n",
    "- **Audio Enhancement**: Time-frequency decomposition techniques can help in separating and isolating specific frequency components of audio signals, enabling the enhancement of desired features or suppression of unwanted noise.\n",
    "\n",
    "By decomposing audio signals into time-frequency representations, researchers and practitioners can gain valuable insights into the temporal and spectral characteristics of the signals, facilitating various audio processing tasks and applications.\n",
    "\n",
    "Overall, time-frequency decomposition is an essential technique in audio processing, enabling a more detailed and comprehensive analysis of audio signals in both the time and frequency domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 5\n",
    "coeffs, freqs = pywt.cwt(audio_data, np.arange(1, levels + 1), 'morl')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(np.abs(coeffs), extent=[0, len(audio_data) / sample_rate, freqs[-1], freqs[0]], aspect='auto', cmap='jet')\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Time-Frequency Decomposition (CWT)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
