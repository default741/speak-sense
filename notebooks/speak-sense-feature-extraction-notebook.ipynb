{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeakSense - Language Detection System (Machine Learning CSCI 6364)\n",
    "\n",
    "**Abde Manaaf Ghadiali (G29583342), Gehna Ahuja (G00000000), Venkatesh Shanmugam (G00000000)**\n",
    "\n",
    "The objective of this project is to develop a robust and accurate system capable of detecting the language spoken in audio recordings. By leveraging advanced machine learning algorithms and signal processing techniques, the system aims to accurately identify the language spoken in various audio inputs, spanning diverse accents, dialects, and environmental conditions. This language detection solution seeks to provide practical applications in speech recognition, transcription, translation, and other fields requiring language-specific processing, thereby enhancing accessibility and usability across linguistic boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an environment for working with audio data, particularly focusing on Indian languages. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Importing Libraries**: Imports necessary libraries for data manipulation, visualization, machine learning, and audio processing.\n",
    "\n",
    "2. **Setting Display Options and Suppressing Warnings**: Configures display options for Pandas and suppresses warnings.\n",
    "\n",
    "3. **Setting Random Seed**: Sets a random seed for reproducibility.\n",
    "\n",
    "4. **Downloading Datasets**: Checks if the necessary datasets are downloaded, and if not, downloads them from Kaggle using the OpenDatasets library and organizes them into appropriate directories.\n",
    "\n",
    "5. **Audio Data Processing**: Prepares the audio data for further analysis. This might include feature extraction, preprocessing, and organizing the data for training machine learning models.\n",
    "\n",
    "6. **Machine Learning**: Utilizes machine learning techniques for tasks such as spoken language identification. This involves splitting the data into training and testing sets, building machine learning models (such as Random Forest or Gradient Boosting), evaluating the models, and generating classification reports and confusion matrices.\n",
    "\n",
    "7. **Deep Learning**: Utilizes deep learning techniques, specifically convolutional neural networks (CNNs), for tasks such as spoken language identification. This involves building and training deep learning models using the TensorFlow and Keras libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe = pd.read_csv('../data/model_data/language_dataframe_v1.csv')\n",
    "\n",
    "spoken_languages_train_path_dataset = '../data/spoken_language_identification/train/train/'\n",
    "indian_languages_train_path_dataset = '../data/model_data/chunked_audio/'\n",
    "\n",
    "language_dataframe['language_label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes audio data for Indian languages by chunking the audio files into segments of fixed duration and resampling them to a target sample rate. Here's a breakdown:\n",
    "\n",
    "1. **Defining Parameters**:\n",
    "    - `indian_languages`: A list containing the names of Indian languages.\n",
    "    - `target_sample_rate`: The target sample rate to which the audio will be resampled.\n",
    "    - `audio_duration`: The duration (in seconds) of each audio chunk.\n",
    "\n",
    "2. **Processing Audio Files**:\n",
    "    - It iterates over each Indian language in the `indian_languages` list.\n",
    "    - For each language, it creates a subset DataFrame `lang_dataframe` containing only files for that language.\n",
    "    - It initializes an empty list `audio_data_list` to store audio data for chunking and a variable `chunk_count` to track the number of chunks processed.\n",
    "\n",
    "3. **Chunking Audio Files**:\n",
    "    - It iterates over each file path in `lang_dataframe['file_name']`.\n",
    "    - It loads each audio file using `librosa.load()` with the specified target sample rate.\n",
    "    - It appends the loaded audio data to the `audio_data_list`.\n",
    "    - If the length of `audio_data_list` reaches 1500 (a chosen threshold) or if it's the last file in the subset DataFrame, it proceeds to chunk the audio data.\n",
    "    - It uses TensorFlow's `tf.signal.frame()` function to segment the concatenated audio data into fixed-duration chunks.\n",
    "    - It iterates over each chunk and writes it to a FLAC file in a specified directory, with the filename indicating the chunk number, index within the language, and language name.\n",
    "    - It increments `chunk_count` to track the number of processed chunks and resets `audio_data_list` for the next iteration or language.\n",
    "\n",
    "Overall, this code efficiently processes audio files for Indian languages, chunks them into fixed-duration segments, and saves them to disk for further analysis or use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_languages = ['bengali', 'gujarati', 'hindi', 'kannada', 'malayalam', 'marathi', 'tamil', 'telugu', 'urdu']\n",
    "target_sample_rate = 22050\n",
    "audio_duration = 10\n",
    "\n",
    "for lang in indian_languages:\n",
    "    lang_dataframe = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "    audio_data_list = []\n",
    "    chunk_count = 0\n",
    "\n",
    "    for idx, file_path in tqdm(enumerate(lang_dataframe['file_name'].values), total=len(lang_dataframe), desc=f\"Processing {lang}\"):\n",
    "        audio_data_list.append(librosa.load(file_path, sr=target_sample_rate)[0])\n",
    "\n",
    "        if (len(audio_data_list) >= 1500) or (idx == len(lang_dataframe) - 1):\n",
    "            audio_chunks = tf.signal.frame(np.concatenate(audio_data_list), int(audio_duration * target_sample_rate), int(audio_duration * target_sample_rate), pad_end=True)\n",
    "\n",
    "            for chunk_idx, audio_chunk in enumerate(audio_chunks):\n",
    "                sf.write(file=f'../data/model_data/chunked_audio/{chunk_count}_{chunk_idx}_{lang}.flac', data=audio_chunk, samplerate=target_sample_rate, format='flac')\n",
    "\n",
    "            chunk_count += 1\n",
    "            audio_data_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bengali, sample_rate_bengali = librosa.load('../data/model_data/chunked_audio/0_1_bengali.flac')\n",
    "print(f'Audio Data Sample Rate: {sample_rate_bengali}')\n",
    "print(f'Audio Data: {data_bengali}')\n",
    "\n",
    "ipd.Audio(data=data_bengali, rate=sample_rate_bengali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Calculate Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str) -> tuple:\n",
    "    \"\"\"Loads audio data from a file using librosa.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sample rate and duration of the audio in seconds.\n",
    "               If an error occurs during processing, returns (np.nan, np.nan).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load audio data and sample rate\n",
    "        audio_data, sample_rate = librosa.load(file_name, sr=None)\n",
    "\n",
    "        # Calculate duration of audio in seconds\n",
    "        audio_duration_sec = int(librosa.get_duration(y=audio_data, sr=sample_rate))\n",
    "\n",
    "        return (sample_rate, audio_duration_sec)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error message if an exception occurs during processing\n",
    "        print(f\"Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "        # Return NaN values for sample rate and duration\n",
    "        return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_language_dataframe = pd.DataFrame({'file_name': [spoken_languages_train_path_dataset + file_name for file_name in os.listdir(spoken_languages_train_path_dataset)]})\n",
    "spoken_language_dataframe['language_label'] = spoken_language_dataframe['file_name'].str.split('/', expand=True).iloc[:, 5].str.split('_', expand=True).iloc[:, 0].replace(to_replace={\n",
    "    'de': 'german', 'en': 'english', 'es': 'spanish'})\n",
    "\n",
    "indian_language_dataframe = pd.DataFrame({'file_name': glob.glob(indian_languages_train_path_dataset)})\n",
    "indian_language_dataframe['language_label'] = indian_language_dataframe['file_name'].str.split('\\\\', expand=True).iloc[:, 1].str.lower()\n",
    "\n",
    "indian_language_dataframe = indian_language_dataframe[indian_language_dataframe['language_label'] != 'punjabi']\n",
    "\n",
    "language_dataframe = pd.concat([spoken_language_dataframe, indian_language_dataframe], ignore_index=True)\n",
    "language_dataframe['file_size_kb'] = (language_dataframe['file_name'].apply(lambda x: os.path.getsize(x)) / 1024).round(3)\n",
    "\n",
    "language_dataframe.to_csv('../data/model_data/language_dataframe_v2.csv', index=False)\n",
    "\n",
    "for lang in tqdm(language_dataframe['language_label'].unique(), desc=\"Languages\"):\n",
    "    lang_data = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "    lang_data[['sample_rate', 'audio_duration_sec']] = lang_data['file_name'].apply(lambda file_name: pd.Series(load_data(file_name=file_name)))\n",
    "\n",
    "    lang_data.to_csv(f'../data/model_data/data_subset/language_dataframe_{lang}_v2.csv', index=False)\n",
    "\n",
    "language_dataframe = pd.concat([pd.read_csv(f'../data/model_data/data_subset/language_dataframe_{lang}_v2.csv') for lang in language_dataframe['language_label'].unique()], ignore_index=True).dropna()\n",
    "language_dataframe.to_csv('../data/model_data/language_dataframe_v2.csv', index=False)\n",
    "\n",
    "language_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['language_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['audio_duration_sec'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe_v1 = pd.read_csv('../data/model_data/language_dataframe_v1.csv')\n",
    "language_dataframe_v2 = pd.read_csv('../data/model_data/language_dataframe_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe_v1.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe_v2.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe_v1['audio_duration_sec'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe_v2['audio_duration_sec'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_value_counts = pd.merge(\n",
    "    left=pd.DataFrame(language_dataframe_v1['language_label'].value_counts()).reset_index(),\n",
    "    right=pd.DataFrame(language_dataframe_v2['language_label'].value_counts()).reset_index(),\n",
    "    on='language_label').rename(columns={'language_label': 'Language Labels', 'count_x': 'Before Chunking', 'count_y': 'After Chunking'})\n",
    "\n",
    "lang_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC Mean Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe = pd.read_csv('../data/model_data/language_dataframe_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the mean Mel-frequency cepstral coefficients (MFCC) features for audio files and saves them in CSV format. Here's the breakdown:\n",
    "\n",
    "1. **Defining Function to Calculate MFCC Features**:\n",
    "    - The function `get_mfcc_features_mean` takes a file name as input and returns the mean MFCC features.\n",
    "    - It loads the audio data using `librosa.load()` with a sample rate of 22050 Hz.\n",
    "    - It calculates MFCC features using `librosa.feature.mfcc()` with parameters specifying 40 coefficients (`n_mfcc=40`).\n",
    "    - It calculates the mean of MFCC features over time and returns the result.\n",
    "\n",
    "2. **Processing Audio Files and Calculating MFCC Features**:\n",
    "    - It iterates over each unique language label in the `language_dataframe`.\n",
    "    - For each language, it creates a subset DataFrame `lang_data` containing only files for that language.\n",
    "    - It calculates the MFCC features mean for each audio file in the subset using the `get_mfcc_features_mean` function and applies it to the `'file_name'` column using `apply()`.\n",
    "    - It saves the resulting DataFrame to a CSV file named `mfcc_data_{lang}_v1.csv` in a specified directory.\n",
    "\n",
    "3. **Combining Processed DataFrames and Saving to CSV**:\n",
    "    - It reads each CSV file for each language subset and concatenates them into a single DataFrame `mfcc_feature_mean_dataframe`.\n",
    "    - It drops any rows with NaN values.\n",
    "    - It saves the combined DataFrame to a CSV file named `mfcc_feature_mean_dataframe_v1.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_features_mean(file_name: str) -> tuple:\n",
    "    audio_data, sample_rate = librosa.load(file_name, sr=22050)\n",
    "\n",
    "    return list(np.mean(librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40).T, axis=0))\n",
    "\n",
    "\n",
    "for lang in language_dataframe['language_label'].unique():\n",
    "    print(f'Currently Running: {lang}')\n",
    "\n",
    "    lang_data = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "    lang_data['mfcc_features_mean'] = lang_data['file_name'].apply(lambda file_name: get_mfcc_features_mean(file_name=file_name))\n",
    "\n",
    "    lang_data.to_csv(f'../data/model_data/mfcc_mean_dataframes/mfcc_data_{lang}_v1.csv', index=False)\n",
    "\n",
    "mfcc_feature_mean_dataframe = pd.concat([pd.read_csv(f'../data/model_data/mfcc_mean_dataframes/mfcc_data_{lang}_v1.csv') for lang in language_dataframe['language_label'].unique()], ignore_index=True).dropna()\n",
    "mfcc_feature_mean_dataframe.to_csv('../data/model_data/mfcc_feature_mean_dataframe_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC Matrix Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe = pd.read_csv('../data/model_data/language_dataframe_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the Mel-frequency cepstral coefficients (MFCC) features for audio files and saves them in CSV format. Here's the breakdown:\n",
    "\n",
    "1. **Defining Function to Calculate MFCC Features**:\n",
    "    - The function `get_mfcc_features` takes a file name as input and returns the MFCC features as a list.\n",
    "    - It loads the audio data using `librosa.load()` with a sample rate of 22050 Hz.\n",
    "    - It calculates MFCC features using `librosa.feature.mfcc()` with parameters specifying 40 coefficients (`n_mfcc=40`).\n",
    "    - It converts the MFCC features to a list and returns them.\n",
    "\n",
    "2. **Sampling Data**:\n",
    "    - It downsamples the `language_dataframe` by randomly selecting 500 samples for each language.\n",
    "    - This downsampling is done to reduce computational complexity and ensure balanced representation across languages.\n",
    "\n",
    "3. **Processing Audio Files and Calculating MFCC Features**:\n",
    "    - It iterates over each unique language label in the downsampled `language_dataframe`.\n",
    "    - For each language, it creates a subset DataFrame `lang_data` containing a random sample of 500 files for that language.\n",
    "    - It calculates the MFCC features for each audio file in the subset using the `get_mfcc_features` function and applies it to the `'file_name'` column using `apply()`.\n",
    "    - It saves the resulting DataFrame to a CSV file named `mfcc_data_{lang}_v1.csv` in a specified directory.\n",
    "\n",
    "4. **Combining Processed DataFrames and Saving to CSV**:\n",
    "    - It reads each CSV file for each language subset and concatenates them into a single DataFrame `mfcc_feature_dataframe`.\n",
    "    - It drops any rows with NaN values.\n",
    "    - It saves the combined DataFrame to a CSV file named `mfcc_feature_dataframe_v1.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_features(file_name: str) -> tuple:\n",
    "    audio_data, sample_rate = librosa.load(file_name, sr=22050)\n",
    "\n",
    "    return librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40).tolist()\n",
    "\n",
    "language_dataframe = language_dataframe.groupby('language_label').apply(lambda row: row.sample(n=500)).reset_index(drop=True)\n",
    "\n",
    "for lang in language_dataframe['language_label'].unique():\n",
    "    print(f'Currently Running: {lang}')\n",
    "\n",
    "    lang_data = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "    lang_data['mfcc_features'] = lang_data['file_name'].apply(lambda file_name: get_mfcc_features(file_name=file_name))\n",
    "\n",
    "    lang_data.to_csv(f'../data/model_data/mfcc_dataframes/mfcc_data_{lang}_v1.csv', index=False)\n",
    "\n",
    "mfcc_feature_dataframe = pd.concat([pd.read_csv(f'../data/model_data/mfcc_dataframes/mfcc_data_{lang}_v1.csv') for lang in language_dataframe['language_label'].unique()], ignore_index=True).dropna()\n",
    "mfcc_feature_dataframe.to_csv('../data/model_data/mfcc_feature_dataframe_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectogram Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe = pd.read_csv('../data/model_data/language_dataframe_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet defines a function `save_audio_spectrogram_plot` and uses it to generate and save spectrogram images for audio files. Here's a breakdown:\n",
    "\n",
    "1. **Defining Function to Save Spectrogram**:\n",
    "    - The function `save_audio_spectrogram_plot` takes four parameters: `file_name` (path to the audio file), `idx` (index of the file), `label` (language label of the file), and `figsize` (optional tuple specifying the size of the figure, default is (5, 5)).\n",
    "    - It loads the audio data using `librosa.load()` with the original sample rate.\n",
    "    - It creates a new figure with a specified size and no frame.\n",
    "    - It generates the spectrogram plot using `plt.specgram()`, which computes and plots the spectrogram of the audio data.\n",
    "    - It saves the generated spectrogram plot as a PNG image in a directory corresponding to the language label and with a filename based on the index.\n",
    "\n",
    "2. **Processing Audio Files and Generating Spectrograms**:\n",
    "    - It first downsamples the `language_dataframe` by randomly selecting 500 samples for each language. This is done to reduce computational load and ensure balanced representation across languages.\n",
    "    - It iterates over each unique language label in the downsampled `language_dataframe`.\n",
    "    - For each language, it creates a subset DataFrame `lang_data` containing a random sample of 500 files for that language.\n",
    "    - It iterates over each file in `lang_data['file_name']`.\n",
    "    - If the directory corresponding to the language label does not exist, it creates one.\n",
    "    - It calls the `save_audio_spectrogram_plot` function to generate and save the spectrogram image for each audio file in the language subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio_spectrogram_plot(file_name: str, idx: int, label: str, figsize: tuple = (5, 5)):\n",
    "    audio_data, sample_rate = librosa.load(file_name, sr=None)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    plt.specgram(audio_data, Fs=sample_rate)\n",
    "\n",
    "    fig.savefig(f'../data/model_data/spectogram_images/{label}/{idx}.png', dpi=64)\n",
    "    plt.close()\n",
    "\n",
    "language_dataframe = language_dataframe.groupby('language_label').apply(lambda row: row.sample(n=500)).reset_index(drop=True)\n",
    "\n",
    "for lang in language_dataframe['language_label'].unique():\n",
    "    lang_data = language_dataframe[language_dataframe['language_label'] == lang].copy()\n",
    "\n",
    "    for idx, file_path in tqdm(enumerate(lang_data['file_name'].values), total=len(lang_data), desc=f\"Processing {lang}\"):\n",
    "        if not os.path.exists(f'../data/model_data/spectrogram_images/{lang}'):\n",
    "            os.makedirs(f'../data/model_data/spectrogram_images/{lang}')\n",
    "\n",
    "        save_audio_spectrogram_plot(file_path, idx, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speak-sense-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
