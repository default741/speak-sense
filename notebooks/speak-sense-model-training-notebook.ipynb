{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeakSense - Language Detection System (Machine Learning CSCI 6364)\n",
    "\n",
    "**Abde Manaaf Ghadiali (G29583342), Gehna Ahuja (G00000000), Venkatesh Shanmugam (G00000000)**\n",
    "\n",
    "The objective of this project is to develop a robust and accurate system capable of detecting the language spoken in audio recordings. By leveraging advanced machine learning algorithms and signal processing techniques, the system aims to accurately identify the language spoken in various audio inputs, spanning diverse accents, dialects, and environmental conditions. This language detection solution seeks to provide practical applications in speech recognition, transcription, translation, and other fields requiring language-specific processing, thereby enhancing accessibility and usability across linguistic boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an environment for working with audio data, particularly focusing on Indian languages. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Importing Libraries**: Imports necessary libraries for data manipulation, visualization, machine learning, and audio processing.\n",
    "\n",
    "2. **Setting Display Options and Suppressing Warnings**: Configures display options for Pandas and suppresses warnings.\n",
    "\n",
    "3. **Setting Random Seed**: Sets a random seed for reproducibility.\n",
    "\n",
    "4. **Downloading Datasets**: Checks if the necessary datasets are downloaded, and if not, downloads them from Kaggle using the OpenDatasets library and organizes them into appropriate directories.\n",
    "\n",
    "5. **Audio Data Processing**: Prepares the audio data for further analysis. This might include feature extraction, preprocessing, and organizing the data for training machine learning models.\n",
    "\n",
    "6. **Machine Learning**: Utilizes machine learning techniques for tasks such as spoken language identification. This involves splitting the data into training and testing sets, building machine learning models (such as Random Forest or Gradient Boosting), evaluating the models, and generating classification reports and confusion matrices.\n",
    "\n",
    "7. **Deep Learning**: Utilizes deep learning techniques, specifically convolutional neural networks (CNNs), for tasks such as spoken language identification. This involves building and training deep learning models using the TensorFlow and Keras libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Input, Rescaling\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature_mean_dataframe = pd.read_csv('../data/model_data/mfcc_feature_mean_dataframe_v1.csv', converters={'mfcc_features_mean': pd.eval})\n",
    "mfcc_feature_mean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mfcc_feature_mean_dataframe, mfcc_feature_mean_dataframe['language_label'], stratify=mfcc_feature_mean_dataframe['language_label'], test_size=0.05, random_state=0)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "print(f'\\nTrain Shape: {X_train.shape}, Test Shape: {X_test.shape}')\n",
    "\n",
    "ipd.display(X_train)\n",
    "\n",
    "language_labels_cols = list(pd.get_dummies(X_train['language_label'], dtype=np.int32).columns.values)\n",
    "\n",
    "X_train, X_test = (np.concatenate(X_train['mfcc_features_mean'].values, axis=0).reshape(-1, 40),\n",
    "                          np.concatenate(X_test['mfcc_features_mean'].values, axis=0).reshape(-1, 40))\n",
    "\n",
    "y_train, y_test = (pd.factorize(y_train)[0],\n",
    "                          pd.factorize(y_test)[0])\n",
    "\n",
    "print(f'Train Shape: {X_train.shape}, Test Shape: {X_test.shape}')\n",
    "print(f'Target: Train Shape: {y_train.shape}, Test Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_object.fit_transform(X_train)\n",
    "X_test_scaled = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training RFC and GBC on Unscaled Data!')\n",
    "rfc_model = RandomForestClassifier(n_estimators=10, n_jobs=-1).fit(X=X_train, y=y_train)\n",
    "gbc_model = GradientBoostingClassifier(n_estimators=10).fit(X=X_train, y=y_train)\n",
    "\n",
    "print('Training RFC and GBC on Scaled Data!')\n",
    "rfc_model_scaled = RandomForestClassifier(n_estimators=10, n_jobs=-1).fit(X=X_train_scaled, y=y_train)\n",
    "gbc_model_scaled = GradientBoostingClassifier(n_estimators=10).fit(X=X_train_scaled, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = confusion_matrix(y_test, rfc_model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = confusion_matrix(y_test, gbc_model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = confusion_matrix(y_test, rfc_model.predict(X_test_scaled))\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data = confusion_matrix(y_test, gbc_model.predict(X_test_scaled))\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(list(y_test), list(rfc_model.predict_proba(X_test)), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(list(y_test), list(gbc_model.predict_proba(X_test)), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(list(y_test), list(rfc_model_scaled.predict_proba(X_test_scaled)), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(list(y_test), list(gbc_model_scaled.predict_proba(X_test_scaled)), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rfc_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, gbc_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rfc_model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, gbc_model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature_mean_dataframe = pd.read_csv('../data/model_data/mfcc_feature_mean_dataframe_v1.csv', converters={'mfcc_features_mean': pd.eval})\n",
    "mfcc_feature_mean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mfcc_feature_mean_dataframe, mfcc_feature_mean_dataframe['language_label'], stratify=mfcc_feature_mean_dataframe['language_label'], test_size=0.03, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, X_train['language_label'], stratify=X_train['language_label'], test_size=0.03, random_state=0)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "print(f'\\nTrain Shape: {X_train.shape}, Validation Shape: {X_val.shape}, Test Shape: {X_test.shape}')\n",
    "\n",
    "ipd.display(X_train)\n",
    "\n",
    "language_labels_cols = list(pd.get_dummies(X_train['language_label'], dtype=np.int32).columns.values)\n",
    "\n",
    "X_train, X_val, X_test = (np.concatenate(X_train['mfcc_features_mean'].values, axis=0).reshape(-1, 40, 1),\n",
    "                          np.concatenate(X_val['mfcc_features_mean'].values, axis=0).reshape(-1, 40, 1),\n",
    "                          np.concatenate(X_test['mfcc_features_mean'].values, axis=0).reshape(-1, 40, 1))\n",
    "\n",
    "y_train, y_val, y_test = (pd.get_dummies(y_train.values, dtype=np.int32).values,\n",
    "                          pd.get_dummies(y_val.values, dtype=np.int32).values,\n",
    "                          pd.get_dummies(y_test.values, dtype=np.int32).values)\n",
    "\n",
    "print(f'Train Shape: {X_train.shape}, Validation Shape: {X_val.shape}, Test Shape: {X_test.shape}')\n",
    "print(f'Target: Train Shape: {y_train.shape}, Validation Shape: {y_val.shape}, Test Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_object.fit_transform(X_train.reshape(-1, 40)).reshape(-1, 40, 1)\n",
    "X_val_scaled = scaler_object.transform(X_val.reshape(-1, 40)).reshape(-1, 40, 1)\n",
    "X_test_scaled = scaler_object.transform(X_test.reshape(-1, 40)).reshape(-1, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dense(input_shape: tuple, output_shape: int) -> object:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_dense(input_shape=(40, ), output_shape=y_train.shape[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def learning_rate_decay(epoch: int) -> float:\n",
    "\treturn 0.00158 * math.pow(0.9, math.floor((1 + epoch) / 1))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('../data/models/language_detection_model_unscaled_v1.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "learning_rate_callback = LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train, epochs=20, verbose=1, batch_size=32, callbacks=[checkpoint_callback, learning_rate_callback],\n",
    "    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_dense(input_shape=(40, ), output_shape=y_train.shape[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def learning_rate_decay(epoch: int) -> float:\n",
    "\treturn 0.00158 * math.pow(0.9, math.floor((1 + epoch) / 1))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('../data/models/language_detection_model_scaled_v1.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "learning_rate_callback = LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train_scaled, y_train, epochs=20, verbose=1, batch_size=32, callbacks=[checkpoint_callback, learning_rate_callback],\n",
    "    validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../data/models/language_detection_model_unscaled_v1.keras')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_argmax = [np.argmax(y_test[i,:]) for i in range(0, len(y_test))]\n",
    "y_pred_argmax = [np.argmax(y_pred[i,:]) for i in range(0,len(y_pred))]\n",
    "\n",
    "confusion_matrix_data = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_test_argmax)\n",
    "\n",
    "y_test_lb = label_binarizer.transform(y_test_argmax)\n",
    "y_pred_lb = label_binarizer.transform(y_pred_argmax)\n",
    "\n",
    "roc_auc_score(list(y_test_lb), list(y_pred_lb), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_argmax, y_pred_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())\n",
    "\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Training vs Validation Model Accuracy')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Training vs Validation Model Loss')\n",
    "plt.ylabel('Loss [CCE]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../data/models/language_detection_model_scaled_v1.keras')\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "y_test_argmax = [np.argmax(y_test[i,:]) for i in range(0, len(y_test))]\n",
    "y_pred_argmax = [np.argmax(y_pred[i,:]) for i in range(0,len(y_pred))]\n",
    "\n",
    "confusion_matrix_data = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_test_argmax)\n",
    "\n",
    "y_test_lb = label_binarizer.transform(y_test_argmax)\n",
    "y_pred_lb = label_binarizer.transform(y_pred_argmax)\n",
    "\n",
    "roc_auc_score(list(y_test_lb), list(y_pred_lb), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_argmax, y_pred_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())\n",
    "\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Training vs Validation Model Accuracy')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Training vs Validation Model Loss')\n",
    "plt.ylabel('Loss [CCE]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature_dataframe = pd.read_csv('../data/model_data/mfcc_feature_dataframe_v1.csv', converters={'mfcc_features': pd.eval}, verbose=2, chunksize=10)\n",
    "\n",
    "for chunk in mfcc_feature_dataframe:\n",
    "    mfcc_feature_dataframe = chunk\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mfcc_feature_dataframe, mfcc_feature_dataframe['language_label'], stratify=mfcc_feature_dataframe['language_label'], test_size=0.05, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, X_train['language_label'], stratify=X_train['language_label'], test_size=0.05, random_state=0)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "print(f'\\nTrain Shape: {X_train.shape}, Validation Shape: {X_val.shape}, Test Shape: {X_test.shape}')\n",
    "\n",
    "ipd.display(X_train)\n",
    "\n",
    "language_labels_cols = list(pd.get_dummies(X_train['language_label'], dtype=np.int32).columns.values)\n",
    "\n",
    "X_train, X_val, X_test = (np.concatenate(X_train['mfcc_features'].values, axis=0).reshape(-1, 40, 431, 1),\n",
    "                          np.concatenate(X_val['mfcc_features'].values, axis=0).reshape(-1, 40, 431, 1),\n",
    "                          np.concatenate(X_test['mfcc_features'].values, axis=0).reshape(-1, 40, 431, 1))\n",
    "\n",
    "y_train, y_val, y_test = (pd.get_dummies(y_train.values, dtype=np.int32).values,\n",
    "                          pd.get_dummies(y_val.values, dtype=np.int32).values,\n",
    "                          pd.get_dummies(y_test.values, dtype=np.int32).values)\n",
    "\n",
    "print(f'Train Shape: {X_train.shape}, Validation Shape: {X_val.shape}, Test Shape: {X_test.shape}')\n",
    "print(f'Target: Train Shape: {y_train.shape}, Validation Shape: {y_val.shape}, Test Shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_object.fit_transform(X_train.reshape(-1, 40 * 431)).reshape(-1, 40, 431, 1)\n",
    "X_val_scaled = scaler_object.transform(X_val.reshape(-1, 40 * 431)).reshape(-1, 40, 431, 1)\n",
    "X_test_scaled = scaler_object.transform(X_test.reshape(-1, 40 * 431)).reshape(-1, 40, 431, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn(input_shape: tuple, output_shape: int) -> object:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (7, 7), activation='relu', padding='valid', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_cnn(input_shape=(40, 431, 1), output_shape=y_train.shape[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def learning_rate_decay(epoch: int) -> float:\n",
    "\treturn 0.00158 * math.pow(0.9, math.floor((1 + epoch) / 1))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('../data/models/language_detection_model_unscaled_v2.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "learning_rate_callback = LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train, epochs=5, verbose=1, batch_size=32, callbacks=[checkpoint_callback, learning_rate_callback],\n",
    "    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_cnn(input_shape=(40, 431, 1), output_shape=y_train.shape[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def learning_rate_decay(epoch: int) -> float:\n",
    "\treturn 0.00158 * math.pow(0.9, math.floor((1 + epoch) / 1))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('../data/models/language_detection_model_scaled_v2.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "learning_rate_callback = LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train_scaled, y_train, epochs=5, verbose=1, batch_size=32, callbacks=[checkpoint_callback, learning_rate_callback],\n",
    "    validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../data/models/language_detection_model_unscaled_v2.keras')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_argmax = [np.argmax(y_test[i,:]) for i in range(0, len(y_test))]\n",
    "y_pred_argmax = [np.argmax(y_pred[i,:]) for i in range(0,len(y_pred))]\n",
    "\n",
    "confusion_matrix_data = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_test_argmax)\n",
    "\n",
    "y_test_lb = label_binarizer.transform(y_test_argmax)\n",
    "y_pred_lb = label_binarizer.transform(y_pred_argmax)\n",
    "\n",
    "roc_auc_score(list(y_test_lb), list(y_pred_lb), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_argmax, y_pred_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())\n",
    "\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Training vs Validation Model Accuracy')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Training vs Validation Model Loss')\n",
    "plt.ylabel('Loss [CCE]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../data/models/language_detection_model_scaled_v2.keras')\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "y_test_argmax = [np.argmax(y_test[i,:]) for i in range(0, len(y_test))]\n",
    "y_pred_argmax = [np.argmax(y_pred[i,:]) for i in range(0,len(y_pred))]\n",
    "\n",
    "confusion_matrix_data = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=language_labels_cols)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_test_argmax)\n",
    "\n",
    "y_test_lb = label_binarizer.transform(y_test_argmax)\n",
    "y_pred_lb = label_binarizer.transform(y_pred_argmax)\n",
    "\n",
    "roc_auc_score(list(y_test_lb), list(y_pred_lb), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_argmax, y_pred_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())\n",
    "\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Training vs Validation Model Accuracy')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Training vs Validation Model Loss')\n",
    "plt.ylabel('Loss [CCE]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Spectogram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram_images = image_dataset_from_directory(\"../data/model_data/spectogram_images\", labels=\"inferred\", image_size=(64, 64), batch_size=None, shuffle=True)\n",
    "class_names = spectogram_images.class_names\n",
    "\n",
    "test_val_records = 100\n",
    "\n",
    "test_images = spectogram_images.take(test_val_records)\n",
    "train_images = spectogram_images.skip(test_val_records)\n",
    "\n",
    "val_images = train_images.take(test_val_records)\n",
    "train_images = train_images.skip(test_val_records)\n",
    "\n",
    "size = (64, 64)\n",
    "\n",
    "train_data = train_images.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "val_data = val_images.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_data = test_images.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "\n",
    "print(f'Train Data Size: {len([i for i, v in enumerate(train_data.as_numpy_iterator())])}')\n",
    "print(f'Test Data Size: {len([i for i, v in enumerate(val_data.as_numpy_iterator())])}')\n",
    "print(f'Test Data Size: {len([i for i, v in enumerate(test_data.as_numpy_iterator())])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([(images, labels) for (images, labels) in train_data.as_numpy_iterator()])\n",
    "val_data = np.array([(images, labels) for (images, labels) in val_data.as_numpy_iterator()])\n",
    "test_data = np.array([(images, labels) for (images, labels) in test_data.as_numpy_iterator()])\n",
    "\n",
    "train_image_data, train_labels = np.concatenate(train_data[:, 0], axis=0).reshape(-1, 64, 64, 3), pd.get_dummies(train_data[:, 1], dtype=np.int32).values\n",
    "val_image_data, val_labels = np.concatenate(val_data[:, 0], axis=0).reshape(-1, 64, 64, 3), pd.get_dummies(val_data[:, 1], dtype=np.int32).values\n",
    "test_image_data, test_labels = np.concatenate(test_data[:, 0], axis=0).reshape(-1, 64, 64, 3), pd.get_dummies(test_data[:, 1], dtype=np.int32).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(train_image_data[0]).astype('uint8'))\n",
    "class_names[np.argmax(train_labels[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn_spectrogram(output_shape: int) -> object:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(64, 64, 3), name='input'))\n",
    "    model.add(Rescaling(1./255))\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_cnn_spectrogram(output_shape=12)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "def learning_rate_decay(epoch: int) -> float:\n",
    "\treturn 0.00158 * math.pow(0.9, math.floor((1 + epoch) / 1))\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('../data/models/language_detection_model_raw_v3.keras', monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "learning_rate_callback = LearningRateScheduler(learning_rate_decay)\n",
    "\n",
    "model_history = model.fit(\n",
    "    train_image_data, train_labels, epochs=20, verbose=1, batch_size=32, callbacks=[checkpoint_callback, learning_rate_callback],\n",
    "    validation_data=(val_image_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../data/models/language_detection_model_raw_v3.keras')\n",
    "model.evaluate(test_image_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_image_data)\n",
    "\n",
    "y_test_argmax = [np.argmax(test_labels[i,:]) for i in range(0, len(test_labels))]\n",
    "y_pred_argmax = [np.argmax(y_pred[i,:]) for i in range(0, len(test_labels))]\n",
    "\n",
    "confusion_matrix_data = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix_data, display_labels=class_names)\n",
    "cmd.plot()\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "print(classification_report(y_test_argmax, y_pred_argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_test_argmax)\n",
    "\n",
    "y_test_lb = label_binarizer.transform(y_test_argmax)\n",
    "y_pred_lb = label_binarizer.transform(y_pred_argmax)\n",
    "\n",
    "roc_auc_score(list(y_test_lb), list(y_pred_lb), multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())\n",
    "\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Training vs Validation Model Accuracy')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Training vs Validation Model Loss')\n",
    "plt.ylabel('Loss [CCE]')\n",
    "plt.xlabel('Epoch [number]')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speak-sense-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
